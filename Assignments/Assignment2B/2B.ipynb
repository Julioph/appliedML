{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### People: \n",
    "Mihkel Sildnik and Julio Ponte Hernandez\n",
    "### The exercice question:\n",
    "Both the perceptron and the LinearSVC are linear classifiers, i.e they separate the classes in this case using a line\n",
    "(the domain is 2-dimensional).\n",
    "If we have the city on one axis and the month on another axis then in the first case the two classes are easily separable.\n",
    "In the other case this is no longer true, there is no line that can perfectly separate the two classes. The model did not\n",
    "\"memorize\" the solution in the first case, the given data was just separable with a line.\n",
    "\n",
    "### Accuracies of the SVC and LogReg classifiers\n",
    "The accuracy depends on if the pipeline uses SelectKBest and if the TfidfVectorizer has the ngram parameter.\n",
    "There is also quite a large difference in training time. Sparse means if the Bonus part 1\n",
    "has been implemented. The accuracies and times are shown in the following table:\n",
    "\n",
    "<table>\n",
    "<tr>    \n",
    "    <th>Sparse\n",
    "    <th>Classifier\n",
    "    <th>Ngram(1,2)\n",
    "    <th>SelectK\n",
    "    <th>Tr. time\n",
    "    <th>Acc\n",
    " <tr>\n",
    "    <td>Yes\n",
    "    <td>SVC\n",
    "    <td>No\n",
    "    <td>Yes\n",
    "    <td>5.2610\n",
    "    <td>0.8309\n",
    "<tr>\n",
    "    <td>Yes\n",
    "    <td>SVC\n",
    "    <td>Yes\n",
    "    <td>No\n",
    "    <td>10.8350\n",
    "    <td>0.8699\n",
    "<tr>\n",
    "    <td>Yes\n",
    "    <td>SVC\n",
    "    <td>Yes\n",
    "    <td>Yes\n",
    "    <td>9.8584\n",
    "    <td>0.8498\n",
    "<tr>\n",
    "    <td>Yes\n",
    "    <td>SVC\n",
    "    <td>No\n",
    "    <td>No\n",
    "    <td>6.5302\n",
    "    <td>0.8397\n",
    "<tr>\n",
    "    <td>No\n",
    "    <td>SVC\n",
    "    <td>No\n",
    "    <td>Yes\n",
    "    <td>4.3324\n",
    "    <td>0.8296\n",
    "<tr>\n",
    "    <td>No\n",
    "    <td>SVC\n",
    "    <td>Yes\n",
    "    <td>No\n",
    "    <td>Mem. error\n",
    "    <td>Mem. error\n",
    "<tr>\n",
    "    <td>No\n",
    "    <td>SVC\n",
    "    <td>Yes\n",
    "    <td>Yes\n",
    "    <td>7.4920\n",
    "    <td>0.8498\n",
    "<tr>\n",
    "    <td>No\n",
    "    <td>SVC\n",
    "    <td>No\n",
    "    <td>No\n",
    "    <td>29.5709\n",
    "    <td>0.8410\n",
    "<tr>\n",
    "    <td>No\n",
    "    <td>Log\n",
    "    <td>No\n",
    "    <td>Yes\n",
    "    <td>7.1579\n",
    "    <td>0.8347\n",
    "<tr>\n",
    "    <td>No\n",
    "    <td>Log\n",
    "    <td>Yes\n",
    "    <td>No\n",
    "    <td>Mem. error\n",
    "    <td>Mem. error\n",
    "<tr>\n",
    "    <td>No\n",
    "    <td>Log\n",
    "    <td>Yes\n",
    "    <td>Yes\n",
    "    <td>11.1218\n",
    "    <td>0.8418\n",
    "<tr>\n",
    "    <td>No\n",
    "    <td>Log\n",
    "    <td>No\n",
    "    <td>No\n",
    "    <td>50.9464\n",
    "    <td>0.8330\n",
    "<tr>\n",
    "    <td>Yes\n",
    "    <td>Log\n",
    "    <td>No\n",
    "    <td>Yes\n",
    "    <td>25.3921\n",
    "    <td>0.8347\n",
    "<tr>\n",
    "    <td>Yes\n",
    "    <td>Log\n",
    "    <td>Yes\n",
    "    <td>No\n",
    "    <td>28.5616\n",
    "    <td>0.8435\n",
    "<tr>\n",
    "    <td>Yes\n",
    "    <td>Log\n",
    "    <td>Yes\n",
    "    <td>Yes\n",
    "    <td>27.6550\n",
    "    <td>0.8418\n",
    "<tr>\n",
    "    <td>Yes\n",
    "    <td>Log\n",
    "    <td>No\n",
    "    <td>No\n",
    "    <td>24.8993\n",
    "    <td>0.8326\n",
    "</table>\n",
    "\n",
    "### Information needed to run the code\n",
    "\n",
    "The only thing that could break it is if the data is not in the same place with regards to the code. We have it in a folder\n",
    "called 'data' that is in the folder that has the code. The classifiers ending in \"Dense\", is our code before we implemented \n",
    "bonus task 1. We also feel that given a person has read the clarification document\n",
    "and the pegasus document the code is written 'clean' enough to need no additional comments beforehand.\n",
    "\n",
    "### BLAS\n",
    "\n",
    "I (Mihkel) found that dscal and daxpy from the blas library didn't give even nearly the same results. All of my debugging was done\n",
    "on dscal and I found that for some reason, the function just returned the same array as was given in, without any\n",
    "multiplication, while I used it during training. My debugging was also made harder that for some reason, sometimes if you call dscal multiple times and\n",
    "assign the value to different variables, the values are all updated. Testing in a console, it seemed that the first couple of \n",
    "calls did not have this and gave correct results but after trying with different multipliers eventually the newly created arrays were \"linked\" and if you\n",
    "called dscal again the old arrays got assigned the same value. I also saw that if I had two calls of dscal in the training loop\n",
    "the first call did nothing and the second one did calculate it, seemingly correctly as well. This behaviour was too erradic for me to trust it in that form.\n",
    "Daxpy aswell gave greatly different results and I did not look into it further after my horrendous experience with dscal. \n",
    "Also, exactly what function is implemented in fortran inside of blas? The documentation for SciPy just says it is a wrapper for\n",
    "\"dscal\" which is implemented in fortran but I could not find what code exactly was being run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from SVClassifier import SVClassifier\n",
    "import scipy.linalg.blas as blas\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "def read_multiclass_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            y, _, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "        return X, Y   \n",
    "    \n",
    "class LinearClassifier(BaseEstimator):\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        # Self.w is a 1d ndarray, X is a csr matrix\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = self.decision_function(X)\n",
    "        # X is a csr matrix\n",
    "        out = np.select([scores >= 0.0, scores < 0.0], [self.positive_class, self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_output(self, Y):\n",
    "        encoded = np.array([1 if y == self.positive_class else -1 for y in Y])\n",
    "        return encoded\n",
    "    \n",
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    if len(x.indices) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return blas.ddot(w[x.indices], x.data)\n",
    "\n",
    "\n",
    "class SVClassifier(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter):\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y, regularization_param):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the SVC learning algorithm.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        Ye = self.encode_output(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        #if not isinstance(X, np.ndarray):\n",
    "        #   X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        \n",
    "        XY = list(zip(X, Ye))\n",
    "        # start iterations\n",
    "        t = 1\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            a = 1\n",
    "            for x_i, y_i in XY:\n",
    "                t += 1\n",
    "                # Calculate steplength\n",
    "                eta = 1 / (regularization_param * t)\n",
    "                # Calculate score\n",
    "                score = sparse_dense_dot(x_i, self.w) * a # ddot does work and is a bit faster\n",
    "                # weight_blas = blas.dscal((1 - eta * regularization_param), self.w) # For some reason this calculation completely fails\n",
    "                # While debugging it also had the fun effect of when calling it again and assigning the value to some new variable\n",
    "                # The ndarray of the previous variable is also updated\n",
    "                #daxpy = blas.daxpy(x_i, weight_helper, a = product) # This function also does not work\n",
    "                if y_i * score < 1.0:\n",
    "                    #self.w = (1 - eta * regularization_param) * self.w\n",
    "                    a = (1 - eta * regularization_param) * a\n",
    "                    add_sparse_to_dense(x_i, self.w, eta * y_i / a)\n",
    "                else:\n",
    "                    #self.w = (1 - eta * regularization_param) * self.w\n",
    "                    a = (1 - eta * regularization_param) * a\n",
    "            self.w = a * self.w\n",
    "\n",
    "class LogClassifier(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter):\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y, regularization_param):\n",
    "        \"\"\"\n",
    "        Train a logistic classifier using the Pegasos algorithm.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        Ye = self.encode_output(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        #if not isinstance(X, np.ndarray):\n",
    "        #    X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # start iterations\n",
    "        t = 1\n",
    "        for i in range(self.n_iter):\n",
    "            a = 1\n",
    "            for x_i, y_i in zip(X, Ye):\n",
    "                t += 1\n",
    "                # Calculate steplength\n",
    "                eta = 1 / (regularization_param * t)\n",
    "                z = sparse_dense_dot(x_i, self.w)\n",
    "                # Calculate score\n",
    "                a = (1 - eta * regularization_param) * a\n",
    "                add_sparse_to_dense(x_i, self.w, eta * y_i / (a * (1 + np.exp(y_i * z))))\n",
    "            self.w = a * self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def time_a_pipeline(pipeline, name_of_classifier):\n",
    "    t4 = time.time()\n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    regularization_param = {name_of_classifier + \"__regularization_param\":1 / len(Xtrain)}\n",
    "    #Train the classifier (adjust weights) and time it\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain, **regularization_param)\n",
    "    t1 = time.time()\n",
    "    #Evaluate on the test set\n",
    "    t2 = time.time()\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    t3 = time.time()\n",
    "    t5 = time.time()\n",
    "    print('Training duration: {:.4f} seconds.'.format(t1 - t0))\n",
    "    print('Prediction duration: {:.4f} seconds.'.format(t3 - t2))\n",
    "    print('Program duration: {:.4f} seconds.\\n'.format(t5 - t4))\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SVClassifierDense(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter, regularization_param): #added regularization_param\n",
    "        self.n_iter = n_iter\n",
    "        self.regularization_param = regularization_param\n",
    "\n",
    "    def fit(self, X, Y): #removed ,regularization_param\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the SVC learning algorithm.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        Ye = self.encode_output(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # start iterations\n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "\n",
    "            for x_i, y_i in zip(X, Ye):\n",
    "                t += 1\n",
    "\n",
    "                # Calculate steplength\n",
    "                eta = 1 / (self.regularization_param * t)\n",
    "                # Calculate score\n",
    "                score = x_i.dot(self.w)\n",
    "\n",
    "                if y_i * score < 1.0:\n",
    "                    self.w = ((1 - eta * self.regularization_param) * self.w) + ((eta * y_i) * x_i)\n",
    "                else:\n",
    "                    self.w = (1 - eta * self.regularization_param) * self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LogClassifierDense(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter, regularization_param): #added regularization_param\n",
    "        self.n_iter = n_iter\n",
    "        self.regularization_param = regularization_param\n",
    "\n",
    "    def fit(self, X, Y): #removed ,regularization_param\n",
    "        \"\"\"\n",
    "        Train a logistic classifier using the Pegasos algorithm.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        Ye = self.encode_output(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # start iterations\n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "\n",
    "            for x_i, y_i in zip(X, Ye):\n",
    "                t += 1\n",
    "\n",
    "                # Calculate steplength\n",
    "                eta = 1 / (self.regularization_param * t)\n",
    "                z = np.dot(self.w, x_i)\n",
    "                # Calculate score\n",
    "                self.w = ((1 - eta * self.regularization_param) * self.w) + eta * (y_i / (1 + np.exp(y_i * z)) * x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ngram, select k = 1000\n",
      "Training duration: 5.2610 seconds.\n",
      "Prediction duration: 0.2523 seconds.\n",
      "Program duration: 5.5462 seconds.\n",
      "\n",
      "Accuracy: 0.8309.\n",
      "With ngram, wo select\n",
      "Training duration: 10.8350 seconds.\n",
      "Prediction duration: 0.8039 seconds.\n",
      "Program duration: 11.6668 seconds.\n",
      "\n",
      "Accuracy: 0.8699.\n",
      "With ngram and select\n",
      "Training duration: 9.8584 seconds.\n",
      "Prediction duration: 0.8148 seconds.\n",
      "Program duration: 10.7082 seconds.\n",
      "\n",
      "Accuracy: 0.8498.\n",
      "Without ngram and select\n",
      "Training duration: 6.5302 seconds.\n",
      "Prediction duration: 0.3152 seconds.\n",
      "Program duration: 6.8774 seconds.\n",
      "\n",
      "Accuracy: 0.8397.\n"
     ]
    }
   ],
   "source": [
    "print(\"sparse,svc\")\n",
    "print(\"No ngram, select k = 1000\")\n",
    "svc_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        SVClassifier(n_iter=35))\n",
    "\n",
    "time_a_pipeline(svc_pipeline, 'svclassifier')\n",
    "print(\"With ngram, wo select\")\n",
    "svc_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(1,2)), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        #SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        SVClassifier(n_iter=35))\n",
    "\n",
    "time_a_pipeline(svc_pipeline, 'svclassifier')\n",
    "print(\"With ngram and select\")\n",
    "svc_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(1,2)), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        SVClassifier(n_iter=35))\n",
    "\n",
    "time_a_pipeline(svc_pipeline, 'svclassifier')\n",
    "print(\"Without ngram and select\")\n",
    "svc_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        #SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        SVClassifier(n_iter=35))\n",
    "\n",
    "time_a_pipeline(svc_pipeline, 'svclassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense\n",
      "No ngram, select k = 1000\n",
      "Training duration: 4.3324 seconds.\n",
      "Prediction duration: 0.2772 seconds.\n",
      "Program duration: 4.6536 seconds.\n",
      "\n",
      "Accuracy: 0.8296.\n",
      "With ngram, wo select\n",
      "Memory Error\n",
      "With ngram and select\n",
      "Training duration: 7.4920 seconds.\n",
      "Prediction duration: 0.6831 seconds.\n",
      "Program duration: 8.2110 seconds.\n",
      "\n",
      "Accuracy: 0.8498.\n",
      "Without ngram and select\n",
      "Training duration: 29.5709 seconds.\n",
      "Prediction duration: 0.3032 seconds.\n",
      "Program duration: 29.9060 seconds.\n",
      "\n",
      "Accuracy: 0.8410.\n"
     ]
    }
   ],
   "source": [
    "print(\"dense,svc\")\n",
    "print(\"No ngram, select k = 1000\")\n",
    "svc_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        SVClassifierDense(n_iter=35))\n",
    "\n",
    "time_a_pipeline(svc_pipeline, 'svclassifierdense')\n",
    "print(\"With ngram, wo select\")\n",
    "print(\"Memory Error \\n\")\n",
    "\"\"\"\n",
    "svc_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(1,2)), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        #SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        SVClassifierDense(n_iter=35))\n",
    "\n",
    "time_a_pipeline(svc_pipeline, 'svclassifierdense')\n",
    "\"\"\"\n",
    "print(\"With ngram and select\")\n",
    "svc_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(1,2)), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        SVClassifierDense(n_iter=35))\n",
    "\n",
    "time_a_pipeline(svc_pipeline, 'svclassifierdense')\n",
    "print(\"Without ngram and select\")\n",
    "svc_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        #SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        SVClassifierDense(n_iter=35))\n",
    "\n",
    "time_a_pipeline(svc_pipeline, 'svclassifierdense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense,log\n",
      "No ngram, select k = 1000\n",
      "Training duration: 7.1579 seconds.\n",
      "Prediction duration: 0.2593 seconds.\n",
      "Program duration: 7.4571 seconds.\n",
      "\n",
      "Accuracy: 0.8347.\n",
      "With ngram, wo select\n",
      "Memory error\n",
      "\n",
      "With ngram and select\n",
      "Training duration: 11.1218 seconds.\n",
      "Prediction duration: 0.7261 seconds.\n",
      "Program duration: 11.8818 seconds.\n",
      "\n",
      "Accuracy: 0.8418.\n",
      "Without ngram and select\n",
      "Training duration: 50.9464 seconds.\n",
      "Prediction duration: 0.2872 seconds.\n",
      "Program duration: 51.2655 seconds.\n",
      "\n",
      "Accuracy: 0.8330.\n"
     ]
    }
   ],
   "source": [
    "print(\"dense,log\")\n",
    "print(\"No ngram, select k = 1000\")\n",
    "log_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        LogClassifierDense(n_iter=35))\n",
    "\n",
    "time_a_pipeline(log_pipeline, 'logclassifierdense')\n",
    "print(\"With ngram, wo select\")\n",
    "print(\"Memory error\\n\")\n",
    "\"\"\"\n",
    "log_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(1,2)), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        #SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        LogClassifierDense(n_iter=35))\n",
    "\n",
    "time_a_pipeline(log_pipeline, 'logclassifierdense')\n",
    "\"\"\"\n",
    "print(\"With ngram and select\")\n",
    "log_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(1,2)), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        LogClassifierDense(n_iter=35))\n",
    "\n",
    "time_a_pipeline(log_pipeline, 'logclassifierdense')\n",
    "print(\"Without ngram and select\")\n",
    "log_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        #SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        LogClassifierDense(n_iter=35))\n",
    "\n",
    "time_a_pipeline(log_pipeline, 'logclassifierdense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse,log\n",
      "No ngram, select k = 1000\n",
      "Training duration: 25.3921 seconds.\n",
      "Prediction duration: 0.2942 seconds.\n",
      "Program duration: 25.7183 seconds.\n",
      "\n",
      "Accuracy: 0.8347.\n",
      "With ngram, wo select\n",
      "Training duration: 28.5616 seconds.\n",
      "Prediction duration: 0.6613 seconds.\n",
      "Program duration: 29.2568 seconds.\n",
      "\n",
      "Accuracy: 0.8435.\n",
      "With ngram and select\n",
      "Training duration: 27.6550 seconds.\n",
      "Prediction duration: 0.6802 seconds.\n",
      "Program duration: 28.3641 seconds.\n",
      "\n",
      "Accuracy: 0.8418.\n",
      "Without ngram and select\n",
      "Training duration: 24.8993 seconds.\n",
      "Prediction duration: 0.2653 seconds.\n",
      "Program duration: 25.1946 seconds.\n",
      "\n",
      "Accuracy: 0.8326.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sildn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\sildn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\sildn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\sildn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "print(\"sparse,log\")\n",
    "print(\"No ngram, select k = 1000\")\n",
    "log_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        LogClassifier(n_iter=35))\n",
    "\n",
    "time_a_pipeline(log_pipeline, 'logclassifier')\n",
    "print(\"With ngram, wo select\")\n",
    "log_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(1,2)), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        #SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        LogClassifier(n_iter=35))\n",
    "\n",
    "time_a_pipeline(log_pipeline, 'logclassifier')\n",
    "print(\"With ngram and select\")\n",
    "log_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(1,2)), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        LogClassifier(n_iter=35))\n",
    "\n",
    "time_a_pipeline(log_pipeline, 'logclassifier')\n",
    "print(\"Without ngram and select\")\n",
    "log_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(), # ngram adds a couple seconds, and around 0.03 to acc\n",
    "        #SelectKBest(k=1000), #The removal of this increases time, small increase of accuracy\n",
    "        Normalizer(),\n",
    "        LogClassifier(n_iter=35))\n",
    "\n",
    "time_a_pipeline(log_pipeline, 'logclassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Bonus task 2\n",
    "#### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One versus rest classifier. SVCDense\n",
      "Accuracy: 0.9295.\n",
      "One versus rest classifier. LogDense\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JulioPHX\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9303.\n"
     ]
    }
   ],
   "source": [
    "X, Y = read_multiclass_data('pa2b/data/all_sentiment_shuffled.txt')\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "#First, use scikit learn classes OnevRest \n",
    "OvR_SVC_pipeline = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            OneVsRestClassifier(SVClassifierDense(n_iter=35, regularization_param=1/len(Xtrain)))\n",
    "            )\n",
    "\n",
    "OvR_Log_pipeline = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            OneVsRestClassifier(LogClassifierDense(n_iter=35, regularization_param=1/len(Xtrain)))\n",
    "            )\n",
    "\n",
    "print(\"One versus rest classifier. SVCDense\")\n",
    "#Train the classifier (adjust weights)\n",
    "OvR_SVC_pipeline.fit(Xtrain, Ytrain)\n",
    "#Evaluate on the test set\n",
    "Yguess = OvR_SVC_pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n",
    "print(\"One versus rest classifier. LogDense\")\n",
    "OvR_Log_pipeline.fit(Xtrain, Ytrain)\n",
    "Yguess = OvR_Log_pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One versus One classifier. SVCDense\n",
      "Accuracy: 0.9198.\n",
      "One versus One classifier. LogDense\n",
      "Accuracy: 0.9236.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JulioPHX\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:36: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "#Then use scikit learn classes OnevOne \n",
    "OvO_SVC_pipeline = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            OneVsOneClassifier(SVClassifierDense(n_iter=35, regularization_param=1/len(Xtrain)))\n",
    "            )\n",
    "\n",
    "OvO_Log_pipeline = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            OneVsOneClassifier(LogClassifierDense(n_iter=35, regularization_param=1/len(Xtrain)))\n",
    "            )\n",
    "\n",
    "print(\"One versus One classifier. SVCDense\")\n",
    "#Train the classifier (adjust weights)\n",
    "OvO_SVC_pipeline.fit(Xtrain, Ytrain)\n",
    "#Evaluate on the test set\n",
    "Yguess = OvO_SVC_pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n",
    "print(\"One versus One classifier. LogDense\")\n",
    "OvO_Log_pipeline.fit(Xtrain, Ytrain)\n",
    "Yguess = OvO_Log_pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "### Bonus task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SVClassifierDenseTorch(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter, regularization_param, optimizer): #added regularization_param\n",
    "        self.n_iter = n_iter\n",
    "        self.regularization_param = regularization_param\n",
    "        self.eta = 0.1\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def adjust_learning_rate(self, optimizer, t):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 1 / (self.regularization_param * t)\n",
    "\n",
    "    def fit(self, X, Y): #removed , regularization_param\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the SVC learning algorithm.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        Ye = self.encode_output(Y)\n",
    "        \n",
    "        X = X.toarray()\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        Ye = torch.tensor(Ye, dtype=torch.float)\n",
    "       \n",
    "        n_features = X.shape[1]\n",
    "        self.w = torch.zeros(n_features, requires_grad=True)\n",
    "        \n",
    "        self.history = []\n",
    "\n",
    "        \n",
    "        optimizer = self.optimizer([self.w], lr=self.eta)\n",
    "\n",
    "        # start iterations\n",
    "        t = 0\n",
    "        # total_loss = 0\n",
    "        for i in range(self.n_iter):\n",
    "            total_loss = 0\n",
    "            for x_i, y_i in zip(X, Ye):\n",
    "                t += 1\n",
    "                loss = 0\n",
    "                \n",
    "                # Calculate steplength\n",
    "                self.adjust_learning_rate(optimizer, t)\n",
    "                # self.eta = 1 / (self.regularization_param * t)\n",
    "                \n",
    "                # Calculate score\n",
    "                score = x_i.dot(self.w)\n",
    "                Error = score - y_i\n",
    "                \n",
    "                # reset all gradients\n",
    "                optimizer.zero_grad()   \n",
    "                \n",
    "                if y_i * score < 1.0:\n",
    "                    loss = 1.0 - (y_i * (score))\n",
    "                    total_loss += loss\n",
    "                    # compute the gradients for the loss for this batch\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    loss = 0.0\n",
    "                    total_loss += loss\n",
    "                    optimizer.step()\n",
    "                    \n",
    "            print(\"Loss: \", loss.item())\n",
    "            self.history.append(loss.item())\n",
    "            \n",
    "        # print('Final loss: {:.4f}'.format(total_loss))\n",
    "        plt.plot(self.history)\n",
    "        plt.title(\"Loss plot\")\n",
    "        #In order for LinearClassifier to predict:\n",
    "        self.w = self.w.detach().numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense,SVC\n",
      "Loss:  549.7230224609375\n",
      "Loss:  522.2447509765625\n",
      "Loss:  507.0074462890625\n",
      "Loss:  495.8825378417969\n",
      "Loss:  487.7752685546875\n",
      "Loss:  481.15325927734375\n",
      "Loss:  475.6102600097656\n",
      "Loss:  470.8914794921875\n",
      "Loss:  466.7762145996094\n",
      "Loss:  463.13995361328125\n",
      "Loss:  459.9100036621094\n",
      "Loss:  456.9339294433594\n",
      "Loss:  454.1985778808594\n",
      "Loss:  451.66326904296875\n",
      "Loss:  449.3046875\n",
      "Loss:  447.09747314453125\n",
      "Loss:  445.03240966796875\n",
      "Loss:  443.10272216796875\n",
      "Loss:  441.242431640625\n",
      "Loss:  439.5121765136719\n",
      "Loss:  437.8874206542969\n",
      "Loss:  436.3409729003906\n",
      "Loss:  434.8825988769531\n",
      "Loss:  433.4919738769531\n",
      "Loss:  432.1564636230469\n",
      "Loss:  430.8669738769531\n",
      "Loss:  429.64715576171875\n",
      "Loss:  428.4791259765625\n",
      "Loss:  427.37353515625\n",
      "Loss:  426.3057556152344\n",
      "Loss:  425.266357421875\n",
      "Loss:  424.2646484375\n",
      "Loss:  423.3013916015625\n",
      "Loss:  422.3685302734375\n",
      "Loss:  421.4760437011719\n",
      "Training duration: 65.5325 seconds.\n",
      "Accuracy: 0.6240.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n",
       "<svg height=\"261.959875pt\" version=\"1.1\" viewBox=\"0 0 371.88369 261.959875\" width=\"371.88369pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       " <defs>\r\n",
       "  <style type=\"text/css\">\r\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\r\n",
       "  </style>\r\n",
       " </defs>\r\n",
       " <g id=\"figure_1\">\r\n",
       "  <g id=\"patch_1\">\r\n",
       "   <path d=\"M 0 261.959875 \r\n",
       "L 371.88369 261.959875 \r\n",
       "L 371.88369 0 \r\n",
       "L 0 0 \r\n",
       "z\r\n",
       "\" style=\"fill:none;\"/>\r\n",
       "  </g>\r\n",
       "  <g id=\"axes_1\">\r\n",
       "   <g id=\"patch_2\">\r\n",
       "    <path d=\"M 29.7875 241.58175 \r\n",
       "L 364.5875 241.58175 \r\n",
       "L 364.5875 24.14175 \r\n",
       "L 29.7875 24.14175 \r\n",
       "z\r\n",
       "\" style=\"fill:#eeeeee;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_1\">\r\n",
       "    <g id=\"xtick_1\">\r\n",
       "     <g id=\"line2d_1\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 45.005682 241.58175 \r\n",
       "L 45.005682 24.14175 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_2\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L 0 -3.5 \r\n",
       "\" id=\"mfe4b1eb9c9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.005682\" xlink:href=\"#mfe4b1eb9c9\" y=\"241.58175\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_1\">\r\n",
       "      <!-- 0 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 31.78125 66.40625 \r\n",
       "Q 24.171875 66.40625 20.328125 58.90625 \r\n",
       "Q 16.5 51.421875 16.5 36.375 \r\n",
       "Q 16.5 21.390625 20.328125 13.890625 \r\n",
       "Q 24.171875 6.390625 31.78125 6.390625 \r\n",
       "Q 39.453125 6.390625 43.28125 13.890625 \r\n",
       "Q 47.125 21.390625 47.125 36.375 \r\n",
       "Q 47.125 51.421875 43.28125 58.90625 \r\n",
       "Q 39.453125 66.40625 31.78125 66.40625 \r\n",
       "z\r\n",
       "M 31.78125 74.21875 \r\n",
       "Q 44.046875 74.21875 50.515625 64.515625 \r\n",
       "Q 56.984375 54.828125 56.984375 36.375 \r\n",
       "Q 56.984375 17.96875 50.515625 8.265625 \r\n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \r\n",
       "Q 6.59375 17.96875 6.59375 36.375 \r\n",
       "Q 6.59375 54.828125 13.0625 64.515625 \r\n",
       "Q 19.53125 74.21875 31.78125 74.21875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-48\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(41.824432 252.680188)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_2\">\r\n",
       "     <g id=\"line2d_3\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 89.76504 241.58175 \r\n",
       "L 89.76504 24.14175 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_4\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"89.76504\" xlink:href=\"#mfe4b1eb9c9\" y=\"241.58175\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_2\">\r\n",
       "      <!-- 5 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 10.796875 72.90625 \r\n",
       "L 49.515625 72.90625 \r\n",
       "L 49.515625 64.59375 \r\n",
       "L 19.828125 64.59375 \r\n",
       "L 19.828125 46.734375 \r\n",
       "Q 21.96875 47.46875 24.109375 47.828125 \r\n",
       "Q 26.265625 48.1875 28.421875 48.1875 \r\n",
       "Q 40.625 48.1875 47.75 41.5 \r\n",
       "Q 54.890625 34.8125 54.890625 23.390625 \r\n",
       "Q 54.890625 11.625 47.5625 5.09375 \r\n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \r\n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \r\n",
       "Q 12.796875 0.140625 7.71875 1.703125 \r\n",
       "L 7.71875 11.625 \r\n",
       "Q 12.109375 9.234375 16.796875 8.0625 \r\n",
       "Q 21.484375 6.890625 26.703125 6.890625 \r\n",
       "Q 35.15625 6.890625 40.078125 11.328125 \r\n",
       "Q 45.015625 15.765625 45.015625 23.390625 \r\n",
       "Q 45.015625 31 40.078125 35.4375 \r\n",
       "Q 35.15625 39.890625 26.703125 39.890625 \r\n",
       "Q 22.75 39.890625 18.8125 39.015625 \r\n",
       "Q 14.890625 38.140625 10.796875 36.28125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-53\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(86.58379 252.680188)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_3\">\r\n",
       "     <g id=\"line2d_5\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 134.524398 241.58175 \r\n",
       "L 134.524398 24.14175 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_6\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"134.524398\" xlink:href=\"#mfe4b1eb9c9\" y=\"241.58175\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_3\">\r\n",
       "      <!-- 10 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 12.40625 8.296875 \r\n",
       "L 28.515625 8.296875 \r\n",
       "L 28.515625 63.921875 \r\n",
       "L 10.984375 60.40625 \r\n",
       "L 10.984375 69.390625 \r\n",
       "L 28.421875 72.90625 \r\n",
       "L 38.28125 72.90625 \r\n",
       "L 38.28125 8.296875 \r\n",
       "L 54.390625 8.296875 \r\n",
       "L 54.390625 0 \r\n",
       "L 12.40625 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-49\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(128.161898 252.680188)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_4\">\r\n",
       "     <g id=\"line2d_7\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 179.283757 241.58175 \r\n",
       "L 179.283757 24.14175 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_8\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.283757\" xlink:href=\"#mfe4b1eb9c9\" y=\"241.58175\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_4\">\r\n",
       "      <!-- 15 -->\r\n",
       "      <g transform=\"translate(172.921257 252.680188)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_5\">\r\n",
       "     <g id=\"line2d_9\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 224.043115 241.58175 \r\n",
       "L 224.043115 24.14175 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_10\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.043115\" xlink:href=\"#mfe4b1eb9c9\" y=\"241.58175\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_5\">\r\n",
       "      <!-- 20 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 19.1875 8.296875 \r\n",
       "L 53.609375 8.296875 \r\n",
       "L 53.609375 0 \r\n",
       "L 7.328125 0 \r\n",
       "L 7.328125 8.296875 \r\n",
       "Q 12.9375 14.109375 22.625 23.890625 \r\n",
       "Q 32.328125 33.6875 34.8125 36.53125 \r\n",
       "Q 39.546875 41.84375 41.421875 45.53125 \r\n",
       "Q 43.3125 49.21875 43.3125 52.78125 \r\n",
       "Q 43.3125 58.59375 39.234375 62.25 \r\n",
       "Q 35.15625 65.921875 28.609375 65.921875 \r\n",
       "Q 23.96875 65.921875 18.8125 64.3125 \r\n",
       "Q 13.671875 62.703125 7.8125 59.421875 \r\n",
       "L 7.8125 69.390625 \r\n",
       "Q 13.765625 71.78125 18.9375 73 \r\n",
       "Q 24.125 74.21875 28.421875 74.21875 \r\n",
       "Q 39.75 74.21875 46.484375 68.546875 \r\n",
       "Q 53.21875 62.890625 53.21875 53.421875 \r\n",
       "Q 53.21875 48.921875 51.53125 44.890625 \r\n",
       "Q 49.859375 40.875 45.40625 35.40625 \r\n",
       "Q 44.1875 33.984375 37.640625 27.21875 \r\n",
       "Q 31.109375 20.453125 19.1875 8.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-50\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(217.680615 252.680188)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_6\">\r\n",
       "     <g id=\"line2d_11\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 268.802473 241.58175 \r\n",
       "L 268.802473 24.14175 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_12\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.802473\" xlink:href=\"#mfe4b1eb9c9\" y=\"241.58175\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_6\">\r\n",
       "      <!-- 25 -->\r\n",
       "      <g transform=\"translate(262.439973 252.680188)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_7\">\r\n",
       "     <g id=\"line2d_13\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 313.561832 241.58175 \r\n",
       "L 313.561832 24.14175 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_14\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"313.561832\" xlink:href=\"#mfe4b1eb9c9\" y=\"241.58175\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_7\">\r\n",
       "      <!-- 30 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 40.578125 39.3125 \r\n",
       "Q 47.65625 37.796875 51.625 33 \r\n",
       "Q 55.609375 28.21875 55.609375 21.1875 \r\n",
       "Q 55.609375 10.40625 48.1875 4.484375 \r\n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \r\n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \r\n",
       "Q 12.796875 0.390625 7.625 2.203125 \r\n",
       "L 7.625 11.71875 \r\n",
       "Q 11.71875 9.328125 16.59375 8.109375 \r\n",
       "Q 21.484375 6.890625 26.8125 6.890625 \r\n",
       "Q 36.078125 6.890625 40.9375 10.546875 \r\n",
       "Q 45.796875 14.203125 45.796875 21.1875 \r\n",
       "Q 45.796875 27.640625 41.28125 31.265625 \r\n",
       "Q 36.765625 34.90625 28.71875 34.90625 \r\n",
       "L 20.21875 34.90625 \r\n",
       "L 20.21875 43.015625 \r\n",
       "L 29.109375 43.015625 \r\n",
       "Q 36.375 43.015625 40.234375 45.921875 \r\n",
       "Q 44.09375 48.828125 44.09375 54.296875 \r\n",
       "Q 44.09375 59.90625 40.109375 62.90625 \r\n",
       "Q 36.140625 65.921875 28.71875 65.921875 \r\n",
       "Q 24.65625 65.921875 20.015625 65.03125 \r\n",
       "Q 15.375 64.15625 9.8125 62.3125 \r\n",
       "L 9.8125 71.09375 \r\n",
       "Q 15.4375 72.65625 20.34375 73.4375 \r\n",
       "Q 25.25 74.21875 29.59375 74.21875 \r\n",
       "Q 40.828125 74.21875 47.359375 69.109375 \r\n",
       "Q 53.90625 64.015625 53.90625 55.328125 \r\n",
       "Q 53.90625 49.265625 50.4375 45.09375 \r\n",
       "Q 46.96875 40.921875 40.578125 39.3125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-51\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(307.199332 252.680188)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_8\">\r\n",
       "     <g id=\"line2d_15\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 358.32119 241.58175 \r\n",
       "L 358.32119 24.14175 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_16\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"358.32119\" xlink:href=\"#mfe4b1eb9c9\" y=\"241.58175\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_8\">\r\n",
       "      <!-- 35 -->\r\n",
       "      <g transform=\"translate(351.95869 252.680188)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_2\">\r\n",
       "    <g id=\"ytick_1\">\r\n",
       "     <g id=\"line2d_17\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 29.7875 233.973205 \r\n",
       "L 364.5875 233.973205 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_18\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L 3.5 0 \r\n",
       "\" id=\"m547128faf0\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"29.7875\" xlink:href=\"#m547128faf0\" y=\"233.973205\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_9\">\r\n",
       "      <!-- 420 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 37.796875 64.3125 \r\n",
       "L 12.890625 25.390625 \r\n",
       "L 37.796875 25.390625 \r\n",
       "z\r\n",
       "M 35.203125 72.90625 \r\n",
       "L 47.609375 72.90625 \r\n",
       "L 47.609375 25.390625 \r\n",
       "L 58.015625 25.390625 \r\n",
       "L 58.015625 17.1875 \r\n",
       "L 47.609375 17.1875 \r\n",
       "L 47.609375 0 \r\n",
       "L 37.796875 0 \r\n",
       "L 37.796875 17.1875 \r\n",
       "L 4.890625 17.1875 \r\n",
       "L 4.890625 26.703125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-52\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(7.2 237.772424)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_2\">\r\n",
       "     <g id=\"line2d_19\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 29.7875 203.146322 \r\n",
       "L 364.5875 203.146322 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_20\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"29.7875\" xlink:href=\"#m547128faf0\" y=\"203.146322\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_10\">\r\n",
       "      <!-- 440 -->\r\n",
       "      <g transform=\"translate(7.2 206.945541)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_3\">\r\n",
       "     <g id=\"line2d_21\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 29.7875 172.31944 \r\n",
       "L 364.5875 172.31944 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_22\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"29.7875\" xlink:href=\"#m547128faf0\" y=\"172.31944\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_11\">\r\n",
       "      <!-- 460 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 33.015625 40.375 \r\n",
       "Q 26.375 40.375 22.484375 35.828125 \r\n",
       "Q 18.609375 31.296875 18.609375 23.390625 \r\n",
       "Q 18.609375 15.53125 22.484375 10.953125 \r\n",
       "Q 26.375 6.390625 33.015625 6.390625 \r\n",
       "Q 39.65625 6.390625 43.53125 10.953125 \r\n",
       "Q 47.40625 15.53125 47.40625 23.390625 \r\n",
       "Q 47.40625 31.296875 43.53125 35.828125 \r\n",
       "Q 39.65625 40.375 33.015625 40.375 \r\n",
       "z\r\n",
       "M 52.59375 71.296875 \r\n",
       "L 52.59375 62.3125 \r\n",
       "Q 48.875 64.0625 45.09375 64.984375 \r\n",
       "Q 41.3125 65.921875 37.59375 65.921875 \r\n",
       "Q 27.828125 65.921875 22.671875 59.328125 \r\n",
       "Q 17.53125 52.734375 16.796875 39.40625 \r\n",
       "Q 19.671875 43.65625 24.015625 45.921875 \r\n",
       "Q 28.375 48.1875 33.59375 48.1875 \r\n",
       "Q 44.578125 48.1875 50.953125 41.515625 \r\n",
       "Q 57.328125 34.859375 57.328125 23.390625 \r\n",
       "Q 57.328125 12.15625 50.6875 5.359375 \r\n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \r\n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \r\n",
       "Q 6.984375 17.96875 6.984375 36.375 \r\n",
       "Q 6.984375 53.65625 15.1875 63.9375 \r\n",
       "Q 23.390625 74.21875 37.203125 74.21875 \r\n",
       "Q 40.921875 74.21875 44.703125 73.484375 \r\n",
       "Q 48.484375 72.75 52.59375 71.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-54\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(7.2 176.118659)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-54\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_4\">\r\n",
       "     <g id=\"line2d_23\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 29.7875 141.492557 \r\n",
       "L 364.5875 141.492557 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_24\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"29.7875\" xlink:href=\"#m547128faf0\" y=\"141.492557\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_12\">\r\n",
       "      <!-- 480 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 31.78125 34.625 \r\n",
       "Q 24.75 34.625 20.71875 30.859375 \r\n",
       "Q 16.703125 27.09375 16.703125 20.515625 \r\n",
       "Q 16.703125 13.921875 20.71875 10.15625 \r\n",
       "Q 24.75 6.390625 31.78125 6.390625 \r\n",
       "Q 38.8125 6.390625 42.859375 10.171875 \r\n",
       "Q 46.921875 13.96875 46.921875 20.515625 \r\n",
       "Q 46.921875 27.09375 42.890625 30.859375 \r\n",
       "Q 38.875 34.625 31.78125 34.625 \r\n",
       "z\r\n",
       "M 21.921875 38.8125 \r\n",
       "Q 15.578125 40.375 12.03125 44.71875 \r\n",
       "Q 8.5 49.078125 8.5 55.328125 \r\n",
       "Q 8.5 64.0625 14.71875 69.140625 \r\n",
       "Q 20.953125 74.21875 31.78125 74.21875 \r\n",
       "Q 42.671875 74.21875 48.875 69.140625 \r\n",
       "Q 55.078125 64.0625 55.078125 55.328125 \r\n",
       "Q 55.078125 49.078125 51.53125 44.71875 \r\n",
       "Q 48 40.375 41.703125 38.8125 \r\n",
       "Q 48.828125 37.15625 52.796875 32.3125 \r\n",
       "Q 56.78125 27.484375 56.78125 20.515625 \r\n",
       "Q 56.78125 9.90625 50.3125 4.234375 \r\n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.734375 -1.421875 13.25 4.234375 \r\n",
       "Q 6.78125 9.90625 6.78125 20.515625 \r\n",
       "Q 6.78125 27.484375 10.78125 32.3125 \r\n",
       "Q 14.796875 37.15625 21.921875 38.8125 \r\n",
       "z\r\n",
       "M 18.3125 54.390625 \r\n",
       "Q 18.3125 48.734375 21.84375 45.5625 \r\n",
       "Q 25.390625 42.390625 31.78125 42.390625 \r\n",
       "Q 38.140625 42.390625 41.71875 45.5625 \r\n",
       "Q 45.3125 48.734375 45.3125 54.390625 \r\n",
       "Q 45.3125 60.0625 41.71875 63.234375 \r\n",
       "Q 38.140625 66.40625 31.78125 66.40625 \r\n",
       "Q 25.390625 66.40625 21.84375 63.234375 \r\n",
       "Q 18.3125 60.0625 18.3125 54.390625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-56\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(7.2 145.291776)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_5\">\r\n",
       "     <g id=\"line2d_25\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 29.7875 110.665675 \r\n",
       "L 364.5875 110.665675 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_26\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"29.7875\" xlink:href=\"#m547128faf0\" y=\"110.665675\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_13\">\r\n",
       "      <!-- 500 -->\r\n",
       "      <g transform=\"translate(7.2 114.464894)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_6\">\r\n",
       "     <g id=\"line2d_27\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 29.7875 79.838792 \r\n",
       "L 364.5875 79.838792 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_28\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"29.7875\" xlink:href=\"#m547128faf0\" y=\"79.838792\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_14\">\r\n",
       "      <!-- 520 -->\r\n",
       "      <g transform=\"translate(7.2 83.638011)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_7\">\r\n",
       "     <g id=\"line2d_29\">\r\n",
       "      <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 29.7875 49.01191 \r\n",
       "L 364.5875 49.01191 \r\n",
       "\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n",
       "     </g>\r\n",
       "     <g id=\"line2d_30\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"29.7875\" xlink:href=\"#m547128faf0\" y=\"49.01191\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_15\">\r\n",
       "      <!-- 540 -->\r\n",
       "      <g transform=\"translate(7.2 52.811129)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"line2d_31\">\r\n",
       "    <path clip-path=\"url(#p5dc0ca6fb0)\" d=\"M 45.005682 34.025386 \r\n",
       "L 53.957553 76.378859 \r\n",
       "L 62.909425 99.864789 \r\n",
       "L 71.861297 117.012101 \r\n",
       "L 80.813168 129.508193 \r\n",
       "L 89.76504 139.714988 \r\n",
       "L 98.716912 148.258657 \r\n",
       "L 107.668783 155.531922 \r\n",
       "L 116.620655 161.874961 \r\n",
       "L 125.572527 167.479691 \r\n",
       "L 134.524398 172.458155 \r\n",
       "L 143.47627 177.04531 \r\n",
       "L 152.428142 181.261428 \r\n",
       "L 161.380013 185.169211 \r\n",
       "L 170.331885 188.804597 \r\n",
       "L 179.283757 192.206674 \r\n",
       "L 188.235628 195.389647 \r\n",
       "L 197.1875 198.36396 \r\n",
       "L 206.139372 201.231308 \r\n",
       "L 215.091243 203.898226 \r\n",
       "L 224.043115 206.402534 \r\n",
       "L 232.994987 208.786142 \r\n",
       "L 241.946858 211.033999 \r\n",
       "L 250.89873 213.17743 \r\n",
       "L 259.850602 215.235911 \r\n",
       "L 268.802473 217.223459 \r\n",
       "L 277.754345 219.103618 \r\n",
       "L 286.706217 220.903954 \r\n",
       "L 295.658088 222.60805 \r\n",
       "L 304.60996 224.253866 \r\n",
       "L 313.561832 225.855936 \r\n",
       "L 322.513703 227.399914 \r\n",
       "L 331.465575 228.884624 \r\n",
       "L 340.417447 230.322485 \r\n",
       "L 349.369318 231.698114 \r\n",
       "\" style=\"fill:none;stroke:#348abd;stroke-linecap:square;stroke-width:2;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_3\">\r\n",
       "    <path d=\"M 29.7875 241.58175 \r\n",
       "L 29.7875 24.14175 \r\n",
       "\" style=\"fill:none;stroke:#bcbcbc;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_4\">\r\n",
       "    <path d=\"M 364.5875 241.58175 \r\n",
       "L 364.5875 24.14175 \r\n",
       "\" style=\"fill:none;stroke:#bcbcbc;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_5\">\r\n",
       "    <path d=\"M 29.7875 241.58175 \r\n",
       "L 364.5875 241.58175 \r\n",
       "\" style=\"fill:none;stroke:#bcbcbc;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_6\">\r\n",
       "    <path d=\"M 29.7875 24.14175 \r\n",
       "L 364.5875 24.14175 \r\n",
       "\" style=\"fill:none;stroke:#bcbcbc;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"text_16\">\r\n",
       "    <!-- Loss plot -->\r\n",
       "    <defs>\r\n",
       "     <path d=\"M 9.8125 72.90625 \r\n",
       "L 19.671875 72.90625 \r\n",
       "L 19.671875 8.296875 \r\n",
       "L 55.171875 8.296875 \r\n",
       "L 55.171875 0 \r\n",
       "L 9.8125 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-76\"/>\r\n",
       "     <path d=\"M 30.609375 48.390625 \r\n",
       "Q 23.390625 48.390625 19.1875 42.75 \r\n",
       "Q 14.984375 37.109375 14.984375 27.296875 \r\n",
       "Q 14.984375 17.484375 19.15625 11.84375 \r\n",
       "Q 23.34375 6.203125 30.609375 6.203125 \r\n",
       "Q 37.796875 6.203125 41.984375 11.859375 \r\n",
       "Q 46.1875 17.53125 46.1875 27.296875 \r\n",
       "Q 46.1875 37.015625 41.984375 42.703125 \r\n",
       "Q 37.796875 48.390625 30.609375 48.390625 \r\n",
       "z\r\n",
       "M 30.609375 56 \r\n",
       "Q 42.328125 56 49.015625 48.375 \r\n",
       "Q 55.71875 40.765625 55.71875 27.296875 \r\n",
       "Q 55.71875 13.875 49.015625 6.21875 \r\n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \r\n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \r\n",
       "Q 5.515625 13.875 5.515625 27.296875 \r\n",
       "Q 5.515625 40.765625 12.171875 48.375 \r\n",
       "Q 18.84375 56 30.609375 56 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-111\"/>\r\n",
       "     <path d=\"M 44.28125 53.078125 \r\n",
       "L 44.28125 44.578125 \r\n",
       "Q 40.484375 46.53125 36.375 47.5 \r\n",
       "Q 32.28125 48.484375 27.875 48.484375 \r\n",
       "Q 21.1875 48.484375 17.84375 46.4375 \r\n",
       "Q 14.5 44.390625 14.5 40.28125 \r\n",
       "Q 14.5 37.15625 16.890625 35.375 \r\n",
       "Q 19.28125 33.59375 26.515625 31.984375 \r\n",
       "L 29.59375 31.296875 \r\n",
       "Q 39.15625 29.25 43.1875 25.515625 \r\n",
       "Q 47.21875 21.78125 47.21875 15.09375 \r\n",
       "Q 47.21875 7.46875 41.1875 3.015625 \r\n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \r\n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \r\n",
       "Q 10.6875 0.296875 5.421875 2 \r\n",
       "L 5.421875 11.28125 \r\n",
       "Q 10.40625 8.6875 15.234375 7.390625 \r\n",
       "Q 20.0625 6.109375 24.8125 6.109375 \r\n",
       "Q 31.15625 6.109375 34.5625 8.28125 \r\n",
       "Q 37.984375 10.453125 37.984375 14.40625 \r\n",
       "Q 37.984375 18.0625 35.515625 20.015625 \r\n",
       "Q 33.0625 21.96875 24.703125 23.78125 \r\n",
       "L 21.578125 24.515625 \r\n",
       "Q 13.234375 26.265625 9.515625 29.90625 \r\n",
       "Q 5.8125 33.546875 5.8125 39.890625 \r\n",
       "Q 5.8125 47.609375 11.28125 51.796875 \r\n",
       "Q 16.75 56 26.8125 56 \r\n",
       "Q 31.78125 56 36.171875 55.265625 \r\n",
       "Q 40.578125 54.546875 44.28125 53.078125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-115\"/>\r\n",
       "     <path id=\"DejaVuSans-32\"/>\r\n",
       "     <path d=\"M 18.109375 8.203125 \r\n",
       "L 18.109375 -20.796875 \r\n",
       "L 9.078125 -20.796875 \r\n",
       "L 9.078125 54.6875 \r\n",
       "L 18.109375 54.6875 \r\n",
       "L 18.109375 46.390625 \r\n",
       "Q 20.953125 51.265625 25.265625 53.625 \r\n",
       "Q 29.59375 56 35.59375 56 \r\n",
       "Q 45.5625 56 51.78125 48.09375 \r\n",
       "Q 58.015625 40.1875 58.015625 27.296875 \r\n",
       "Q 58.015625 14.40625 51.78125 6.484375 \r\n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \r\n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \r\n",
       "Q 20.953125 3.328125 18.109375 8.203125 \r\n",
       "z\r\n",
       "M 48.6875 27.296875 \r\n",
       "Q 48.6875 37.203125 44.609375 42.84375 \r\n",
       "Q 40.53125 48.484375 33.40625 48.484375 \r\n",
       "Q 26.265625 48.484375 22.1875 42.84375 \r\n",
       "Q 18.109375 37.203125 18.109375 27.296875 \r\n",
       "Q 18.109375 17.390625 22.1875 11.75 \r\n",
       "Q 26.265625 6.109375 33.40625 6.109375 \r\n",
       "Q 40.53125 6.109375 44.609375 11.75 \r\n",
       "Q 48.6875 17.390625 48.6875 27.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-112\"/>\r\n",
       "     <path d=\"M 9.421875 75.984375 \r\n",
       "L 18.40625 75.984375 \r\n",
       "L 18.40625 0 \r\n",
       "L 9.421875 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-108\"/>\r\n",
       "     <path d=\"M 18.3125 70.21875 \r\n",
       "L 18.3125 54.6875 \r\n",
       "L 36.8125 54.6875 \r\n",
       "L 36.8125 47.703125 \r\n",
       "L 18.3125 47.703125 \r\n",
       "L 18.3125 18.015625 \r\n",
       "Q 18.3125 11.328125 20.140625 9.421875 \r\n",
       "Q 21.96875 7.515625 27.59375 7.515625 \r\n",
       "L 36.8125 7.515625 \r\n",
       "L 36.8125 0 \r\n",
       "L 27.59375 0 \r\n",
       "Q 17.1875 0 13.234375 3.875 \r\n",
       "Q 9.28125 7.765625 9.28125 18.015625 \r\n",
       "L 9.28125 47.703125 \r\n",
       "L 2.6875 47.703125 \r\n",
       "L 2.6875 54.6875 \r\n",
       "L 9.28125 54.6875 \r\n",
       "L 9.28125 70.21875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-116\"/>\r\n",
       "    </defs>\r\n",
       "    <g transform=\"translate(165.182375 18.14175)scale(0.144 -0.144)\">\r\n",
       "     <use xlink:href=\"#DejaVuSans-76\"/>\r\n",
       "     <use x=\"55.697266\" xlink:href=\"#DejaVuSans-111\"/>\r\n",
       "     <use x=\"116.878906\" xlink:href=\"#DejaVuSans-115\"/>\r\n",
       "     <use x=\"168.978516\" xlink:href=\"#DejaVuSans-115\"/>\r\n",
       "     <use x=\"221.078125\" xlink:href=\"#DejaVuSans-32\"/>\r\n",
       "     <use x=\"252.865234\" xlink:href=\"#DejaVuSans-112\"/>\r\n",
       "     <use x=\"316.341797\" xlink:href=\"#DejaVuSans-108\"/>\r\n",
       "     <use x=\"344.125\" xlink:href=\"#DejaVuSans-111\"/>\r\n",
       "     <use x=\"405.306641\" xlink:href=\"#DejaVuSans-116\"/>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "  </g>\r\n",
       " </g>\r\n",
       " <defs>\r\n",
       "  <clipPath id=\"p5dc0ca6fb0\">\r\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"29.7875\" y=\"24.14175\"/>\r\n",
       "  </clipPath>\r\n",
       " </defs>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, Y = read_data(\"pa2b/data/all_sentiment_shuffled.txt\")\n",
    "Xtrain, Xtest, Ytrain,Ytest = train_test_split(X,Y,test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dense,SVC\")\n",
    "svctorch_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "        SVClassifierDenseTorch(n_iter=35, regularization_param= 1/len(Xtrain), optimizer=torch.optim.SGD)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "t0 = time.time()\n",
    "svctorch_pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "\n",
    "Yguess = svctorch_pipeline.predict(Xtest)\n",
    "\n",
    "print('Training duration: {:.4f} seconds.'.format(t1 - t0))\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "#### Pytorch optimizers comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "<table>\n",
    "<tr>    \n",
    "    <th>Classifier\n",
    "    <th>Accuracy\n",
    "    <th>Time(seconds)\n",
    "    <th>Final loss\n",
    " <tr>\n",
    "    <td>SVC(dense) SGD\n",
    "    <td>0.6240\n",
    "    <td>65.65\n",
    "    <td>421.476\n",
    "<tr>\n",
    "    <td>SVC(dense) Adam\n",
    "    <td>0.7121\n",
    "    <td>88.83\n",
    "    <td>5039.4521\n",
    "<tr>\n",
    "    <td>SVC(dense) Adagrad\n",
    "    <td>0.5451\n",
    "    <td>86.23\n",
    "    <td>7016.9169\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "The above table shows Accuracy, time and final loss values for the SVC classifier with different optimizer choices. The optimizer choice is set in the last pipeline step. For the Logarithmic regression classifier, we encountered issues with the handling of Pytorch tensors. Specifically, it looks like in pytorch every calculation spawned by a tenser at some point is also a tensor, sometimes requiring to use .item() to get a unique value from the tensor. However, in the log regression code, we could not handle an issue regarding a 'grad' attribute and a grad_fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LogClassifierDenseTorch(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter, regularization_param, optimizer): #added regularization_param\n",
    "        self.n_iter = n_iter\n",
    "        self.regularization_param = regularization_param\n",
    "        self.eta = 0.1\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def adjust_learning_rate(self, optimizer, t):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 1 / (self.regularization_param * t)\n",
    "\n",
    "    def fit(self, X, Y): #removed , regularization_param\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the LogRegression learning algorithm.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        Ye = self.encode_output(Y)\n",
    "        \n",
    "        X = X.toarray()\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        Ye = torch.tensor(Ye, dtype=torch.float)\n",
    "       \n",
    "        n_features = X.shape[1]\n",
    "        self.w = torch.zeros(n_features, requires_grad=True)\n",
    "        \n",
    "        self.history = []\n",
    "\n",
    "        \n",
    "        optimizer = self.optimizer([self.w], lr=self.eta)\n",
    "\n",
    "        # start iterations\n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "            total_loss = 0\n",
    "            for x_i, y_i in zip(X, Ye):\n",
    "                t += 1\n",
    "                loss = 0\n",
    "                \n",
    "                # Calculate steplength\n",
    "                self.adjust_learning_rate(optimizer, t)\n",
    "                # self.eta = 1 / (self.regularization_param * t)\n",
    "                \n",
    "                # Calculate score\n",
    "                z = x_i.dot(self.w)\n",
    "                # Error = score - y_i\n",
    "                # reset all gradients\n",
    "                optimizer.zero_grad()   \n",
    "                \n",
    "                loss = torch.log(1+(torch.exp(-(y_i*z.item()))))\n",
    "                total_loss += loss\n",
    "                                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                    \n",
    "            print(\"Loss: \", loss.item())\n",
    "            self.history.append(loss)\n",
    "            \n",
    "        # print('Final loss: {:.4f}'.format(loss))\n",
    "        plt.plot(self.history)\n",
    "        plt.title(\"Loss plot:\")\n",
    "        #In order for LinearClassifier to predict:\n",
    "        self.w = self.w.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense,log\n",
      "SGD Optimizer\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-ea7504f46019>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mlogtorch_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-136-90bd0a563733>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "print(\"dense,log\")\n",
    "print(\"SGD Optimizer\")\n",
    "logtorch_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "        LogClassifierDenseTorch(n_iter=35, regularization_param= 1/len(Xtrain), optimizer=torch.optim.SGD)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "t0 = time.time()\n",
    "logtorch_pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "\n",
    "Yguess = logtorch_pipeline.predict(Xtest)\n",
    "\n",
    "print('Training duration: {:.4f} seconds.'.format(t1 - t0))\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Assignment4)",
   "language": "python",
   "name": "pycharm-46474081"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
